{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation for Time Series Data\n",
    "- turn a time series graph into a simple graph without time\n",
    "- turn time information into node's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: SIR_nsrc10_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_entire.pickle\n",
      "1: SIR_nsrc7_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_entire.pickle\n",
      "2: SIR_nsrc14_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_torchentire.pickle\n",
      "3: SIR_nsrc14_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_entire.pickle\n",
      "4: SIR_nsrc1_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_entire.pickle\n",
      "5: split\n",
      "6: SIR_nsrc1_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_entire.pickle\n",
      "7: SIR_nsrc10_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_torchentire.pickle\n",
      "8: SIR_nsrc1_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_torchentire.pickle\n",
      "9: SIR_nsrc1_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_torchentire.pickle\n",
      "10: SIR_nsrc7_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_torchentire.pickle\n",
      "11: SIR_nsrc7_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_entire.pickle\n",
      "12: SIR_nsrc1_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_torchentire_with_node_features.pickle\n",
      "13: SIR_nsrc7_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_torchentire.pickle\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/home/twp/lantian/Conformalized-Network-Source-Detection/SD-STGCN/dataset/highSchool/data/SIR'\n",
    "# print out list of files in data_folder\n",
    "files = os.listdir(data_folder)\n",
    "for idx, file in enumerate(files):\n",
    "    print(f\"{idx}: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = files[13]\n",
    "#data_file = 'SIR_nsrc1_Rzero43.44_beta0.25_gamma0.15_T30_ls21200_nf16_torchentire.pickle'\n",
    "\n",
    "data_path = os.path.join(data_folder, data_file)\n",
    "# Load the data\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'tuple'>\n",
      "Data length: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data type: {type(data)}\")  # tuple\n",
    "print(f\"Data length: {len(data)}\") # length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 0: Time series\n",
      "  Type: <class 'torch.Tensor'>\n",
      "  Length: 21200 \n",
      "  Shape: torch.Size([21200, 30, 774]) ([number of simulations], [number of snapshots], [number of nodes])\n",
      "  Unique digits in tensor (sampled): [0, 1] (0 = susceptible, 1 = infected, 2 = recovered)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# view first element\n",
    "i = 0\n",
    "item = data[i]\n",
    "print(f\"Item {i}: Time series\")\n",
    "print(f\"  Type: {type(item)}\")\n",
    "print(f\"  Length: {len(item)} \")\n",
    "print(f\"  Shape: {item.shape} ([number of simulations], [number of snapshots], [number of nodes])\")\n",
    "\n",
    "\n",
    "# Use torch.randint to sample indices directly on the tensor for speed\n",
    "numel = item.numel()\n",
    "num_samples = min(10000, numel)\n",
    "indices = torch.randint(0, numel, (num_samples,))\n",
    "sampled = item.view(-1)[indices]\n",
    "\n",
    "unique_digits = torch.unique(sampled)\n",
    "print(f\"  Unique digits in tensor (sampled): {unique_digits.tolist()} (0 = susceptible, 1 = infected, 2 = recovered)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1: Node labels/ground truth (whether the node is a source or not)\n",
      "  Type: <class 'torch.Tensor'>\n",
      "  Length: 21200 \n",
      "  Shape: torch.Size([21200, 774]) ([number of simulations], [number of nodes])\n",
      "  Unique digits in tensor: [0, 1]\n",
      "21200 simulations have 7 sources\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "item = data[i]\n",
    "print(f\"Item {i}: Node labels/ground truth (whether the node is a source or not)\")\n",
    "print(f\"  Type: {type(item)}\")\n",
    "print(f\"  Length: {len(item)} \")\n",
    "print(f\"  Shape: {item.shape} ([number of simulations], [number of nodes])\")\n",
    "\n",
    "unique_digits = torch.unique(item)\n",
    "print(f\"  Unique digits in tensor: {unique_digits.tolist()}\")\n",
    "\n",
    "# for each simulation, count the number of sources\n",
    "# print out \"{xx} simulations have {yy} sources; {zz} simulations have {ww} sources\"\n",
    "num_simulations = item.shape[0]\n",
    "num_sources = item.sum(dim=1)  # sum across nodes for each simulation\n",
    "source_counts = torch.unique(num_sources, return_counts=True)\n",
    "for count, num_simulations_with_count in zip(*source_counts):\n",
    "    print(f\"{num_simulations_with_count.item()} simulations have {count.item()} sources\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 2: skip idx: the first N snapshots to ignore\n",
      "  Type: <class 'list'>\n",
      "  Length: 21200 \n",
      "  Unique digits in list: [1]\n",
      "  Skip idx: 1 (the first 1 snapshots to ignore)\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "item = data[i]\n",
    "print(f\"Item {i}: skip idx: the first N snapshots to ignore\")\n",
    "print(f\"  Type: {type(item)}\")\n",
    "print(f\"  Length: {len(item)} \")\n",
    "\n",
    "# number of unique digits in the list\n",
    "import numpy as np\n",
    "unique_digits, counts = np.unique(item, return_counts=True)\n",
    "print(f\"  Unique digits in list: {unique_digits.tolist()}\")\n",
    "\n",
    "# set skip_idx to the first element\n",
    "skip_idx = item[0]\n",
    "print(f\"  Skip idx: {skip_idx[0]} (the first {skip_idx[0]} snapshots to ignore)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert time series to features\n",
    "- a function `timeseries_to_features` takes a time series, a skip value, and an optional `nf` value (default is 16). This function converts the time series into a feature vector `[t0, t1, t2]` as follows:\n",
    "    - The time series is a list of integers of length 30, where each value can be 0 (susceptible), 1 (infected), or 2 (recovered). The values are non-decreasing over time.\n",
    "    - Ignore the first `skip` elements of the time series, then consider the next `nf` elements.\n",
    "    - Count the number of 0s, 1s, and 2s in this segment:\n",
    "        - `t0` = number of 0s (susceptible)\n",
    "        - `t1` = number of 1s (infected)\n",
    "        - `t2` = number of 2s (recovered)\n",
    "    - Return the feature vector `[t0, t1, t2]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_to_features(time_series, skip, nf=16):\n",
    "    \"\"\"\n",
    "    Convert a time series into a feature vector [t0, t1, t2].\n",
    "\n",
    "    Args:\n",
    "        time_series (list or np.ndarray): The time series data (length = 30).\n",
    "        skip (int): The number of initial snapshots to ignore.\n",
    "        nf (int): The number of snapshots to consider (default is 16).\n",
    "\n",
    "    Returns:\n",
    "        list: A feature vector [t0, t1, t2], where:\n",
    "              t0 = number of 0s,\n",
    "              t1 = number of 1s,\n",
    "              t2 = number of 2s.\n",
    "    \"\"\"\n",
    "    # Slice the time series based on skip and nf\n",
    "    sliced_series = time_series[skip:skip + nf]\n",
    "\n",
    "    # Count occurrences of 0, 1, and 2\n",
    "    t0 = sliced_series.count(0)\n",
    "    t1 = sliced_series.count(1)\n",
    "    t2 = sliced_series.count(2)\n",
    "\n",
    "    # Return the feature vector\n",
    "    return [t0, t1, t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation 15234:\n",
      "  Source node 104 time series: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [0, 16, 0]\n",
      "  Non-source node 202 time series: \n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [3, 13, 0]\n",
      "  Non-source node 134 time series: \n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [2, 14, 0]\n",
      "\n",
      "Simulation 19599:\n",
      "  Source node 464 time series: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [0, 16, 0]\n",
      "  Non-source node 647 time series: \n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [2, 14, 0]\n",
      "  Non-source node 736 time series: \n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [4, 12, 0]\n",
      "\n",
      "Simulation 2086:\n",
      "  Source node 256 time series: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [0, 16, 0]\n",
      "  Non-source node 32 time series: \n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [3, 13, 0]\n",
      "  Non-source node 749 time series: \n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    Features: [2, 14, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "time_series = data[0]\n",
    "# print some sample node and their time series\n",
    "# Select 3 random simulations\n",
    "random_sims = random.sample(range(time_series.shape[0]), 3)\n",
    "\n",
    "for sim in random_sims:\n",
    "    labels = data[1][sim]  # get labels for this simulation\n",
    "    source_nodes = [i for i, label in enumerate(labels) if label == 1]\n",
    "    non_source_nodes = [i for i, label in enumerate(labels) if label == 0]\n",
    "\n",
    "    print(f\"\\nSimulation {sim}:\")\n",
    "    if source_nodes:\n",
    "        selected_source = random.choice(source_nodes)\n",
    "        ts = time_series[sim, :, selected_source].tolist()\n",
    "        print(f\"  Source node {selected_source} time series: \\n{ts}\")\n",
    "        skip = data[2][sim][0] if isinstance(data[2][sim], list) else data[2][sim]\n",
    "        features = timeseries_to_features(ts, skip)\n",
    "        print(f\"    Features: {features}\")\n",
    "    else:\n",
    "        print(\"  No source nodes in this simulation.\")\n",
    "\n",
    "    if len(non_source_nodes) >= 4:\n",
    "        selected_non_sources = random.sample(non_source_nodes, 2)\n",
    "    else:\n",
    "        selected_non_sources = non_source_nodes\n",
    "\n",
    "    for node in selected_non_sources:\n",
    "        ts = time_series[sim, :, node].tolist()\n",
    "        print(f\"  Non-source node {node} time series: \\n{ts}\")\n",
    "        skip = data[2][sim][0] if isinstance(data[2][sim], list) else data[2][sim]\n",
    "        features = timeseries_to_features(ts, skip)\n",
    "        print(f\"    Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_timeseries_to_features(data, nf=16, normalize=False):\n",
    "    \"\"\"\n",
    "    Convert the time series data from the 'data' tuple into feature vectors for all nodes,\n",
    "    ensuring the output format is [21200, 3, 774].\n",
    "\n",
    "    Args:\n",
    "        data (tuple): The dataset tuple containing:\n",
    "                      - data[0]: Time series (shape: [21200, 30, 774]).\n",
    "                      - data[2]: Skip values (shape: [21200, 1]).\n",
    "        nf (int): The number of snapshots to consider (default is 16).\n",
    "        normalize (bool): Whether to normalize the output values by dividing by nf.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A PyTorch tensor of shape [21200, 3, 774], where:\n",
    "                      - 21200: Number of simulations.\n",
    "                      - 3: Feature vector dimensions [t0, t1, t2].\n",
    "                      - 774: Number of nodes per simulation.\n",
    "    \"\"\"\n",
    "    time_series = data[0]  # Shape: [21200, 30, 774]\n",
    "    skip_values = data[2]  # Shape: [21200, 1]\n",
    "\n",
    "    # Initialize an empty tensor to store features\n",
    "    num_simulations, num_snapshots, num_nodes = time_series.shape\n",
    "    node_features = torch.zeros((num_simulations, 3, num_nodes), dtype=torch.float32)\n",
    "\n",
    "    for sim_idx in range(num_simulations):\n",
    "        skip = skip_values[sim_idx][0]  # Get the skip value for this simulation\n",
    "        sliced_series = time_series[sim_idx, skip:skip + nf, :]  # Slice the time series based on skip and nf\n",
    "\n",
    "        # Count occurrences of 0, 1, and 2 across the sliced series\n",
    "        t0 = (sliced_series == 0).sum(dim=0)  # Count 0s for each node\n",
    "        t1 = (sliced_series == 1).sum(dim=0)  # Count 1s for each node\n",
    "        t2 = (sliced_series == 2).sum(dim=0)  # Count 2s for each node\n",
    "\n",
    "        # Normalize if required\n",
    "        if normalize:\n",
    "            t0 = t0 / nf\n",
    "            t1 = t1 / nf\n",
    "            t2 = t2 / nf\n",
    "\n",
    "        # Store the feature vectors in the output tensor\n",
    "        node_features[sim_idx, 0, :] = t0\n",
    "        node_features[sim_idx, 1, :] = t1\n",
    "        node_features[sim_idx, 2, :] = t2\n",
    "\n",
    "    return node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = convert_timeseries_to_features(data, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features shape: 21200 simulations, 3 nodes, 774 features per node\n",
      "Simulation 0 features: tensor([[0.1250, 0.0625, 0.1250, 0.1250, 0.0625],\n",
      "        [0.8750, 0.9375, 0.8750, 0.8750, 0.9375],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "Simulation 1 features: tensor([[0.1250, 0.0625, 0.0625, 0.1250, 0.0625],\n",
      "        [0.8750, 0.9375, 0.9375, 0.8750, 0.9375],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "Simulation 2 features: tensor([[0.0625, 0.1250, 0.1250, 0.0625, 0.1250],\n",
      "        [0.9375, 0.8750, 0.8750, 0.9375, 0.8750],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "Simulation 3 features: tensor([[0.1875, 0.1250, 0.1250, 0.1875, 0.1875],\n",
      "        [0.8125, 0.8750, 0.8750, 0.8125, 0.8125],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "Simulation 4 features: tensor([[0.1875, 0.1875, 0.1250, 0.1250, 0.1875],\n",
      "        [0.8125, 0.8125, 0.8750, 0.8750, 0.8125],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Node features shape: {len(node_features)} simulations, {len(node_features[0])} nodes, {len(node_features[0][0])} features per node\")\n",
    "# print the first 5 features for the first simulation\n",
    "for i in range(5):\n",
    "    print(f\"Simulation {i} features: {node_features[i, :, :5]}\")  # Print first 5 nodes for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new data with node features to /home/twp/lantian/Conformalized-Network-Source-Detection/SD-STGCN/dataset/highSchool/data/SIR/SIR_nsrc7_Rzero2.5_beta0.25_gamma0_T30_ls21200_nf16_torchentire_with_node_features.pickle\n"
     ]
    }
   ],
   "source": [
    "# Replace data[0] with node_features and save as a new pickle file\n",
    "new_data = (node_features, data[1], data[2])\n",
    "\n",
    "# Define new file name\n",
    "new_data_file = data_file.replace('.pickle', '_with_node_features.pickle')\n",
    "new_data_path = os.path.join(data_folder, new_data_file)\n",
    "\n",
    "# Save the new data tuple\n",
    "with open(new_data_path, 'wb') as f_out:\n",
    "    pickle.dump(new_data, f_out)\n",
    "\n",
    "print(f\"Saved new data with node features to {new_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and assign features to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 774\n",
      "Number of edges: 7992\n",
      "No node features found.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph_folder = '/home/twp/lantian/Conformalized-Network-Source-Detection/SD-STGCN/dataset/highSchool/data/graph'\n",
    "graph_name = 'highSchool.edgelist'\n",
    "\n",
    "# load the graph using networkx\n",
    "graph_path = os.path.join(graph_folder, graph_name)\n",
    "graph = nx.read_edgelist(graph_path)\n",
    "\n",
    "# print the number of nodes and edges\n",
    "print(f\"Number of nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {graph.number_of_edges()}\")\n",
    "\n",
    "# are there any node features?\n",
    "if nx.get_node_attributes(graph, 'feature'):\n",
    "    print(\"Node features found.\")\n",
    "else:\n",
    "    print(\"No node features found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
